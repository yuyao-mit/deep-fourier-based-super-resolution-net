[rank: 0] Seed set to 42
[rank: 3] Seed set to 42
[rank: 7] Seed set to 42
[rank: 1] Seed set to 42
[rank: 2] Seed set to 42
[rank: 4] Seed set to 42
[rank: 5] Seed set to 42
[rank: 6] Seed set to 42
/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/utils/loss.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(vgg_path))
/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/utils/loss.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(vgg_path))
/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/utils/loss.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(vgg_path))
/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/utils/loss.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(vgg_path))
/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/utils/loss.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(vgg_path))
/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/utils/loss.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(vgg_path))
/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/utils/loss.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(vgg_path))
/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/utils/loss.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(vgg_path))
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name    | Type              | Params | Mode 
------------------------------------------------------
0 | net     | DFSR              | 1.0 M  | train
1 | loss_fn | CombinatorialLoss | 14.7 M | train
------------------------------------------------------
1.0 M     Trainable params
14.7 M    Non-trainable params
15.8 M    Total params
63.029    Total estimated model params size (MB)
161       Modules in train mode
32        Modules in eval mode
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Error'
c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorterminate called after throwing an instance of 'c10::Error'
c10::Error'
'
c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
  what():    what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b1edb344446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b1edb2ee6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b1edb25ba18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b1edb25bd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b1ea7315c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b1ea731e735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b1e9081f630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b1edb32569f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b1edb31e37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b1edb31e529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b1e90ae5a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b1e90ae5dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b1edb344446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b1edb2ee6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b1edb25ba18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b1edb25bd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b1ea7315c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b1ea731e735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b1e9081f630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b1edb32569f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ac06e4ad446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ac06e4576e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ac06e3c4a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ac06e3c4d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ac03a47ec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ac03a487735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ac023988630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ac06e48e69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ac06e48737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ac06e487529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ac023c4ea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ac023c4edc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():    what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ac06e4ad446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ac06e4576e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ac06e3c4a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ac06e3c4d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ac03a47ec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ac03a487735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ac023988630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ac06e48e69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b58966e5446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b589668f6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b58965fca18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b58965fcd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b58626b6c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b58626bf735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b584bbc0630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b58966c669f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b58966bf37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b58966bf529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b584be86a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b584be86dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b58966e5446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b589668f6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b58965fca18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b58965fcd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b58626b6c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b58626bf735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b584bbc0630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b58966c669f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b1edb31e37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b1edb31e529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b1e90ae5a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b1e90ae5dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


  what():    what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2f3c195446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b2f3c13f6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2f3c0aca18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2f3c0acd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b2f08166c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2f0816f735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2ef1670630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b2f3c17669f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b2f3c16f37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b2f3c16f529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b2ef1936a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b2ef1936dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2f3c195446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b2f3c13f6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2f3c0aca18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2f3c0acd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b2f08166c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2f0816f735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2ef1670630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b2f3c17669f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ac06e48737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ac06e487529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ac023c4ea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ac023c4edc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ac06e4ad446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ac06e4576e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ac06e3c4a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ac06e3c4d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ac03a47ec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ac03a487735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ac023988630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ac06e48e69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ac06e48737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ac06e487529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ac023c4ea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ac023c4edc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ac06e4ad446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ac06e4576e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ac06e3c4a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ac06e3c4d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b58966bf37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b58966bf529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b584be86a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b584be86dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():    what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b58966e5446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b589668f6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b58965fca18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b58965fcd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b58626b6c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b58626bf735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b584bbc0630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b58966c669f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b58966bf37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b58966bf529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b584be86a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b584be86dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b58966e5446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b589668f6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b58965fca18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b58965fcd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b2f3c16f37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b2f3c16f529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b2ef1936a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b2ef1936dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2f3c195446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b2f3c13f6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2f3c0aca18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2f3c0acd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b2f08166c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2f0816f735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2ef1670630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b2f3c17669f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b2f3c16f37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b2f3c16f529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b2ef1936a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b2ef1936dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2f3c195446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b2f3c13f6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2f3c0aca18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2f3c0acd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ac03a47ec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ac03a487735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ac023988630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ac06e48e69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ac06e48737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ac06e487529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ac023c4ea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ac023c4edc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


frame #4: <unknown function> + 0x1021c4a (0x2b58626b6c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b58626bf735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b584bbc0630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b58966c669f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b58966bf37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b58966bf529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b584be86a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b584be86dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


frame #4: <unknown function> + 0x1021c4a (0x2b2f08166c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2f0816f735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2ef1670630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b2f3c17669f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b2f3c16f37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b2f3c16f529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b2ef1936a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b2ef1936dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():    what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ae680fe3446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ae680f8d6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ae680efaa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ae680efad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ae64cfb4c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ae64cfbd735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ae6364be630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ae680fc469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ae680fbd37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ae680fbd529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ae636784a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ae636784dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ae680fe3446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ae680f8d6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ae680efaa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ae680efad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ae64cfb4c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ae64cfbd735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ae6364be630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ae680fc469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b43e101f446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b43e0fc96e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b43e0f36a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b43e0f36d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b43acff0c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b43acff9735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b43964fa630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b43e100069f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b43e0ff937b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b43e0ff9529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b43967c0a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b43967c0dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b43e101f446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b43e0fc96e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b43e0f36a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b43e0f36d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b43acff0c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b43acff9735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b43964fa630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b43e100069f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():    what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2af45dc01446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2af45dbab6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2af45db18a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2af45db18d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2af429bd2c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2af429bdb735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2af4130dc630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2af45dbe269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2af45dbdb37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2af45dbdb529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2af4133a2a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2af4133a2dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2af45dc01446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2af45dbab6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2af45db18a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2af45db18d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2af429bd2c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2af429bdb735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2af4130dc630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2af45dbe269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ae680fbd37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ae680fbd529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ae636784a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ae636784dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b43e0ff937b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b43e0ff9529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b43967c0a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b43967c0dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2af45dbdb37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2af45dbdb529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2af4133a2a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2af4133a2dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2af45dc01446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2af45dbab6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2af45db18a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2af45db18d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2af429bd2c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2af429bdb735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2af4130dc630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2af45dbe269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2af45dbdb37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2af45dbdb529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2af4133a2a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2af4133a2dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2afe35798446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2afe357426e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2afe356afa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2afe356afd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2afe01769c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2afe01772735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2afdeac73630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2afe3577969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2afe3577237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2afe35772529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2afdeaf39a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2afdeaf39dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2afe35798446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2afe357426e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2afe356afa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2afe356afd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2afe01769c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2afe01772735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2afdeac73630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2afe3577969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ae680fe3446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ae680f8d6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ae680efaa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ae680efad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ae64cfb4c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ae64cfbd735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ae6364be630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ae680fc469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ae680fbd37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ae680fbd529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ae636784a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ae636784dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ae680fe3446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ae680f8d6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ae680efaa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ae680efad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ae64cfb4c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ae64cfbd735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ae6364be630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ae680fc469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b43e101f446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b43e0fc96e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b43e0f36a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b43e0f36d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b43acff0c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b43acff9735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b43964fa630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b43e100069f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b43e0ff937b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b43e0ff9529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b43967c0a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b43967c0dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b43e101f446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b43e0fc96e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b43e0f36a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b43e0f36d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b43acff0c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b43acff9735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b43964fa630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b43e100069f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2af45dc01446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2af45dbab6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2af45db18a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2af45db18d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2af429bd2c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2af429bdb735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2af4130dc630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2af45dbe269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2af45dbdb37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2af45dbdb529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2af4133a2a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2af4133a2dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2afe3577237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2afe35772529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2afdeaf39a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2afdeaf39dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2afe35798446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2afe357426e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2afe356afa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2afe356afd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2afe01769c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2afe01772735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2afdeac73630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2afe3577969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2afe3577237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2afe35772529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2afdeaf39a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2afdeaf39dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ae680fbd37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ae680fbd529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ae636784a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ae636784dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b43e0ff937b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b43e0ff9529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b43967c0a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b43967c0dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2afe35798446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2afe357426e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2afe356afa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2afe356afd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2afe01769c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2afe01772735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2afdeac73630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2afe3577969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2afe3577237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2afe35772529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2afdeaf39a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2afdeaf39dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

[rank3]: Traceback (most recent call last):
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank3]:     data = self._data_queue.get(timeout=timeout)
[rank3]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank3]:     self.not_empty.wait(remaining)
[rank3]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank3]:     gotit = waiter.acquire(True, timeout)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank3]:     _error_if_any_worker_fails()
[rank3]: RuntimeError: DataLoader worker (pid 29673) is killed by signal: Aborted. 

[rank3]: The above exception was the direct cause of the following exception:

[rank3]: Traceback (most recent call last):
[rank3]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 65, in <module>
[rank3]:     main()
[rank3]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 62, in main
[rank3]:     trainer.fit(model, datamodule=dm)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank3]:     call._call_and_handle_interrupt(
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank3]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank3]:     return function(*args, **kwargs)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank3]:     self._run(model, ckpt_path=ckpt_path)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank3]:     results = self._run_stage()
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank3]:     self.fit_loop.run()
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank3]:     self.advance()
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank3]:     self.epoch_loop.run(self._data_fetcher)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank4]:     data = self._data_queue.get(timeout=timeout)
[rank4]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank4]:     self.not_empty.wait(remaining)
[rank4]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank4]:     gotit = waiter.acquire(True, timeout)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank4]:     _error_if_any_worker_fails()
[rank4]: RuntimeError: DataLoader worker (pid 10124) is killed by signal: Aborted. 

[rank4]: The above exception was the direct cause of the following exception:

[rank4]: Traceback (most recent call last):
[rank3]:     self.advance(data_fetcher)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank3]:     batch, _, __ = next(data_fetcher)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank3]:     batch = super().__next__()
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank3]:     batch = next(self.iterator)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank3]:     out = next(self._iterator)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank3]:     out[i] = next(self.iterators[i])
[rank4]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 65, in <module>
[rank4]:     main()
[rank4]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 62, in main
[rank4]:     trainer.fit(model, datamodule=dm)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank4]:     call._call_and_handle_interrupt(
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank4]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank4]:     return function(*args, **kwargs)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank3]:     data = self._next_data()
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank4]:     self._run(model, ckpt_path=ckpt_path)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank4]:     results = self._run_stage()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank4]:     self.fit_loop.run()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank4]:     self.advance()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank4]:     self.epoch_loop.run(self._data_fetcher)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank3]:     idx, data = self._get_data()
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank3]:     success, data = self._try_get_data()
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank3]:     raise RuntimeError(
[rank3]: RuntimeError: DataLoader worker (pid(s) 29666, 29670, 29673, 29676) exited unexpectedly
[rank4]:     self.advance(data_fetcher)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank4]:     batch, _, __ = next(data_fetcher)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank4]:     batch = super().__next__()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank4]:     batch = next(self.iterator)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank4]:     out = next(self._iterator)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank4]:     out[i] = next(self.iterators[i])
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank4]:     data = self._next_data()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank6]:     data = self._data_queue.get(timeout=timeout)
[rank6]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank6]:     self.not_empty.wait(remaining)
[rank6]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank6]:     gotit = waiter.acquire(True, timeout)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank6]:     _error_if_any_worker_fails()
[rank6]: RuntimeError: DataLoader worker (pid 10130) is killed by signal: Aborted. 

[rank6]: The above exception was the direct cause of the following exception:

[rank6]: Traceback (most recent call last):
[rank6]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 65, in <module>
[rank6]:     main()
[rank6]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 62, in main
[rank6]:     trainer.fit(model, datamodule=dm)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank6]:     call._call_and_handle_interrupt(
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank6]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank6]:     return function(*args, **kwargs)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank6]:     self._run(model, ckpt_path=ckpt_path)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank6]:     results = self._run_stage()
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank6]:     self.fit_loop.run()
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank6]:     self.advance()
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank6]:     self.epoch_loop.run(self._data_fetcher)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank6]:     self.advance(data_fetcher)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank6]:     batch, _, __ = next(data_fetcher)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank6]:     batch = super().__next__()
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank6]:     batch = next(self.iterator)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank6]:     out = next(self._iterator)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank6]:     out[i] = next(self.iterators[i])
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank6]:     data = self._next_data()
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank7]:     data = self._data_queue.get(timeout=timeout)
[rank7]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank7]:     self.not_empty.wait(remaining)
[rank7]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank7]:     gotit = waiter.acquire(True, timeout)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank7]:     _error_if_any_worker_fails()
[rank7]: RuntimeError: DataLoader worker (pid 10125) is killed by signal: Aborted. 

[rank7]: The above exception was the direct cause of the following exception:

[rank7]: Traceback (most recent call last):
[rank7]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 65, in <module>
[rank7]:     main()
[rank7]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 62, in main
[rank7]:     trainer.fit(model, datamodule=dm)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank7]:     call._call_and_handle_interrupt(
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank7]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank7]:     return function(*args, **kwargs)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank7]:     self._run(model, ckpt_path=ckpt_path)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank7]:     results = self._run_stage()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank7]:     self.fit_loop.run()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank7]:     self.advance()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank7]:     self.epoch_loop.run(self._data_fetcher)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank7]:     self.advance(data_fetcher)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank7]:     batch, _, __ = next(data_fetcher)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank7]:     batch = super().__next__()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank7]:     batch = next(self.iterator)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank7]:     out = next(self._iterator)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank7]:     out[i] = next(self.iterators[i])
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank7]:     data = self._next_data()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank4]:     idx, data = self._get_data()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank4]:     success, data = self._try_get_data()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank4]:     raise RuntimeError(
[rank4]: RuntimeError: DataLoader worker (pid(s) 10124, 10128, 10132, 10137) exited unexpectedly
[rank6]:     idx, data = self._get_data()
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank6]:     success, data = self._try_get_data()
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank6]:     raise RuntimeError(
[rank6]: RuntimeError: DataLoader worker (pid(s) 10126, 10130, 10134, 10139) exited unexpectedly
[rank7]:     idx, data = self._get_data()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank7]:     success, data = self._try_get_data()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank7]:     raise RuntimeError(
[rank7]: RuntimeError: DataLoader worker (pid(s) 10125, 10129, 10133, 10138) exited unexpectedly
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank5]:     data = self._data_queue.get(timeout=timeout)
[rank5]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank5]:     self.not_empty.wait(remaining)
[rank5]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank5]:     gotit = waiter.acquire(True, timeout)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank5]:     _error_if_any_worker_fails()
[rank5]: RuntimeError: DataLoader worker (pid 10123) is killed by signal: Aborted. 

[rank5]: The above exception was the direct cause of the following exception:

[rank5]: Traceback (most recent call last):
[rank5]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 65, in <module>
[rank5]:     main()
[rank5]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 62, in main
[rank5]:     trainer.fit(model, datamodule=dm)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank5]:     call._call_and_handle_interrupt(
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank5]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank5]:     return function(*args, **kwargs)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank5]:     self._run(model, ckpt_path=ckpt_path)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank5]:     results = self._run_stage()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank5]:     self.fit_loop.run()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank5]:     self.advance()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank5]:     self.epoch_loop.run(self._data_fetcher)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank5]:     self.advance(data_fetcher)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank5]:     batch, _, __ = next(data_fetcher)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank5]:     batch = super().__next__()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank5]:     batch = next(self.iterator)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank5]:     out = next(self._iterator)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank5]:     out[i] = next(self.iterators[i])
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank5]:     data = self._next_data()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank1]:     data = self._data_queue.get(timeout=timeout)
[rank1]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank1]:     self.not_empty.wait(remaining)
[rank1]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank1]:     gotit = waiter.acquire(True, timeout)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank1]:     _error_if_any_worker_fails()
[rank1]: RuntimeError: DataLoader worker (pid 29678) is killed by signal: Aborted. 

[rank1]: The above exception was the direct cause of the following exception:

[rank1]: Traceback (most recent call last):
[rank5]:     idx, data = self._get_data()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank5]:     success, data = self._try_get_data()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank5]:     raise RuntimeError(
[rank5]: RuntimeError: DataLoader worker (pid(s) 10123, 10127, 10131, 10135) exited unexpectedly
[rank1]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 65, in <module>
[rank1]:     main()
[rank1]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 62, in main
[rank1]:     trainer.fit(model, datamodule=dm)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank1]:     call._call_and_handle_interrupt(
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank1]:     return function(*args, **kwargs)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank1]:     self._run(model, ckpt_path=ckpt_path)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank1]:     results = self._run_stage()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank1]:     self.fit_loop.run()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank1]:     self.advance()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank1]:     self.epoch_loop.run(self._data_fetcher)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank1]:     self.advance(data_fetcher)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank1]:     batch, _, __ = next(data_fetcher)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank1]:     batch = super().__next__()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank1]:     batch = next(self.iterator)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank1]:     out = next(self._iterator)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank1]:     out[i] = next(self.iterators[i])
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank1]:     data = self._next_data()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank1]:     idx, data = self._get_data()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank1]:     success, data = self._try_get_data()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank1]:     raise RuntimeError(
[rank1]: RuntimeError: DataLoader worker (pid(s) 29667, 29671, 29674, 29678) exited unexpectedly
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank0]:     data = self._data_queue.get(timeout=timeout)
[rank0]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank0]:     self.not_empty.wait(remaining)
[rank0]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank0]:     gotit = waiter.acquire(True, timeout)
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank0]:     _error_if_any_worker_fails()
[rank0]: RuntimeError: DataLoader worker (pid 29664) is killed by signal: Aborted. 

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 65, in <module>
[rank0]:     main()
[rank0]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 62, in main
[rank0]:     trainer.fit(model, datamodule=dm)
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank0]:     results = self._run_stage()
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank0]:     self.fit_loop.run()
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank0]:     self.advance()
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank0]:     self.epoch_loop.run(self._data_fetcher)
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank0]:     self.advance(data_fetcher)
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank0]:     batch, _, __ = next(data_fetcher)
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank0]:     batch = super().__next__()
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank0]:     batch = next(self.iterator)
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank0]:     out = next(self._iterator)
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank0]:     out[i] = next(self.iterators[i])
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank0]:     idx, data = self._get_data()
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank0]:     success, data = self._try_get_data()
[rank0]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank0]:     raise RuntimeError(
[rank0]: RuntimeError: DataLoader worker (pid(s) 29664, 29668) exited unexpectedly
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank2]:     data = self._data_queue.get(timeout=timeout)
[rank2]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank2]:     self.not_empty.wait(remaining)
[rank2]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank2]:     gotit = waiter.acquire(True, timeout)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank2]:     _error_if_any_worker_fails()
[rank2]: RuntimeError: DataLoader worker (pid 29675) is killed by signal: Aborted. 

[rank2]: The above exception was the direct cause of the following exception:

[rank2]: Traceback (most recent call last):
[rank2]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 65, in <module>
[rank2]:     main()
[rank2]:   File "/work2/10214/yu_yao/frontera/deep-fourier-based-super-resolution-net/src/train_dfsr.py", line 62, in main
[rank2]:     trainer.fit(model, datamodule=dm)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank2]:     call._call_and_handle_interrupt(
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank2]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank2]:     return function(*args, **kwargs)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank2]:     self._run(model, ckpt_path=ckpt_path)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank2]:     results = self._run_stage()
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank2]:     self.fit_loop.run()
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank2]:     self.advance()
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank2]:     self.epoch_loop.run(self._data_fetcher)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank2]:     self.advance(data_fetcher)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank2]:     batch, _, __ = next(data_fetcher)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank2]:     batch = super().__next__()
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank2]:     batch = next(self.iterator)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank2]:     out = next(self._iterator)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank2]:     out[i] = next(self.iterators[i])
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank2]:     data = self._next_data()
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank2]:     idx, data = self._get_data()
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank2]:     success, data = self._try_get_data()
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank2]:     raise RuntimeError(
[rank2]: RuntimeError: DataLoader worker (pid(s) 29665, 29669, 29672, 29675) exited unexpectedly
srun: error: c196-031: task 0: Exited with exit code 1
srun: error: c196-032: task 4: Exited with exit code 1
slurmstepd: error: *** STEP 7094186.0 ON c196-031 CANCELLED AT 2025-05-07T15:56:33 DUE TO TIME LIMIT ***
srun: got SIGCONT
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
srun: forcing job termination
srun: error: c196-032: tasks 5-7: Terminated
srun: error: c196-031: tasks 1-3: Terminated
